{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# YOLOv3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Detecting the bounding box for a Number Plate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the paths to the images and store them in test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os# Current directory\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# print(current_dir)\n",
    "current_dir = 'exp_plate/predict_images/'\n",
    "\n",
    "# Percentage of images to be used for the test set\n",
    "file_test = open('test.txt', 'w')# Populate train.txt and test.txt  \n",
    "for pathAndFilename in glob.iglob(os.path.join(current_dir, \"*.jpg\")):  \n",
    "    title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n",
    "    file_test.write(current_dir+title + '.jpg' + \"\\n\")\n",
    "file_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below command will generate  result.txt which contains the the predictions made by yolo for each of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer     filters    size              input                output\n",
      "   0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32 0.299 BF\n",
      "   1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64 1.595 BF\n",
      "   2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32 0.177 BF\n",
      "   3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64 1.595 BF\n",
      "   4 Shortcut Layer: 1\n",
      "   5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128 1.595 BF\n",
      "   6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF\n",
      "   7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF\n",
      "   8 Shortcut Layer: 5\n",
      "   9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF\n",
      "  10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF\n",
      "  11 Shortcut Layer: 8\n",
      "  12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  15 Shortcut Layer: 12\n",
      "  16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  18 Shortcut Layer: 15\n",
      "  19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  21 Shortcut Layer: 18\n",
      "  22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  24 Shortcut Layer: 21\n",
      "  25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  27 Shortcut Layer: 24\n",
      "  28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  30 Shortcut Layer: 27\n",
      "  31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  33 Shortcut Layer: 30\n",
      "  34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  36 Shortcut Layer: 33\n",
      "  37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  40 Shortcut Layer: 37\n",
      "  41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  43 Shortcut Layer: 40\n",
      "  44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  46 Shortcut Layer: 43\n",
      "  47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  49 Shortcut Layer: 46\n",
      "  50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  52 Shortcut Layer: 49\n",
      "  53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  55 Shortcut Layer: 52\n",
      "  56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  58 Shortcut Layer: 55\n",
      "  59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  61 Shortcut Layer: 58\n",
      "  62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  65 Shortcut Layer: 62\n",
      "  66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  68 Shortcut Layer: 65\n",
      "  69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  71 Shortcut Layer: 68\n",
      "  72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  74 Shortcut Layer: 71\n",
      "  75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  81 conv     18  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x  18 0.006 BF\n",
      "  82 yolo\n",
      "  83 route  79\n",
      "  84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256 0.044 BF\n",
      "  85 upsample            2x    13 x  13 x 256   ->    26 x  26 x 256\n",
      "  86 route  85 61\n",
      "  87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256 0.266 BF\n",
      "  88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  93 conv     18  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  18 0.012 BF\n",
      "  94 yolo\n",
      "  95 route  91\n",
      "  96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128 0.044 BF\n",
      "  97 upsample            2x    26 x  26 x 128   ->    52 x  52 x 128\n",
      "  98 route  97 36\n",
      "  99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128 0.266 BF\n",
      " 100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      " 101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      " 102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      " 103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      " 104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      " 105 conv     18  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x  18 0.025 BF\n",
      " 106 yolo\n",
      "Loading weights from backup_plate/lp_yolov3_final.weights...Done!\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not compiled with OpenCV, saving to predictions.png instead\n",
      "Not compiled with OpenCV, saving to predictions.png instead\n"
     ]
    }
   ],
   "source": [
    "!./darknet detector test data/obj.data cfg/lp_yolov3.cfg backup_plate/lp_yolov3_final.weights -ext_output < test.txt > result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below command will generate the corresponding images with bounding boxes in the directory exp_plate/predicted_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer     filters    size              input                output\n",
      "   0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32 0.299 BF\n",
      "   1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64 1.595 BF\n",
      "   2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32 0.177 BF\n",
      "   3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64 1.595 BF\n",
      "   4 Shortcut Layer: 1\n",
      "   5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128 1.595 BF\n",
      "   6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF\n",
      "   7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF\n",
      "   8 Shortcut Layer: 5\n",
      "   9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF\n",
      "  10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF\n",
      "  11 Shortcut Layer: 8\n",
      "  12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  15 Shortcut Layer: 12\n",
      "  16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  18 Shortcut Layer: 15\n",
      "  19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  21 Shortcut Layer: 18\n",
      "  22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  24 Shortcut Layer: 21\n",
      "  25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  27 Shortcut Layer: 24\n",
      "  28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  30 Shortcut Layer: 27\n",
      "  31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  33 Shortcut Layer: 30\n",
      "  34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      "  35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      "  36 Shortcut Layer: 33\n",
      "  37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  40 Shortcut Layer: 37\n",
      "  41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  43 Shortcut Layer: 40\n",
      "  44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  46 Shortcut Layer: 43\n",
      "  47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  49 Shortcut Layer: 46\n",
      "  50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  52 Shortcut Layer: 49\n",
      "  53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  55 Shortcut Layer: 52\n",
      "  56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  58 Shortcut Layer: 55\n",
      "  59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  61 Shortcut Layer: 58\n",
      "  62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  65 Shortcut Layer: 62\n",
      "  66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  68 Shortcut Layer: 65\n",
      "  69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  71 Shortcut Layer: 68\n",
      "  72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  74 Shortcut Layer: 71\n",
      "  75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n",
      "  80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n",
      "  81 conv     18  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x  18 0.006 BF\n",
      "  82 yolo\n",
      "  83 route  79\n",
      "  84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256 0.044 BF\n",
      "  85 upsample            2x    13 x  13 x 256   ->    26 x  26 x 256\n",
      "  86 route  85 61\n",
      "  87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256 0.266 BF\n",
      "  88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n",
      "  92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n",
      "  93 conv     18  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  18 0.012 BF\n",
      "  94 yolo\n",
      "  95 route  91\n",
      "  96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128 0.044 BF\n",
      "  97 upsample            2x    26 x  26 x 128   ->    52 x  52 x 128\n",
      "  98 route  97 36\n",
      "  99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128 0.266 BF\n",
      " 100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      " 101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      " 102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      " 103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n",
      " 104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n",
      " 105 conv     18  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x  18 0.025 BF\n",
      " 106 yolo\n",
      "Loading weights from backup_plate/lp_yolov3_final.weights...Done!\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_72.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_100.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Dubai_288.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Dubai_251.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/1236.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/1254.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/1237.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_71.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_117.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Dubai_287.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_84.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_68.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Indian_10.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_86.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Indian_280.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Dubai_269.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Dubai_252.png instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_55.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_83.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_54.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/1253.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Dubai_268.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Indian_281.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Indian_27.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Israeli_69.png instead\n",
      "Not compiled with OpenCV, saving to exp_plate/predicted_images/Indian_248.png instead\n"
     ]
    }
   ],
   "source": [
    "!./darknet detector batch data/obj.data cfg/lp_yolov3.cfg backup_plate/lp_yolov3_final.weights batch exp_plate/predict_images/ exp_plate/predicted_images/ > exp_plate/results/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will extract the necessary information associated with the detected classes per image and will store that in the dictionary boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"result.txt\",'r')\n",
    "lines = f.readlines()\n",
    "i = 0\n",
    "boxes = {}\n",
    "filepath = \"\"\n",
    "array = []\n",
    "per = 0\n",
    "lines.pop(len(lines)-1)\n",
    "while(i<len(lines)):\n",
    "    if(lines[i][0]=='E'):\n",
    "        filepath = \"\"\n",
    "        j =18\n",
    "        while(lines[i][j]!=':'):\n",
    "            filepath+=lines[i][j]\n",
    "            j+=1\n",
    "        boxes[filepath] = []\n",
    "    elif(lines[i][0]=='l'):\n",
    "        j = 0\n",
    "        while(lines[i][j]!='('):\n",
    "            if(lines[i][j]==':'):\n",
    "                per = 0\n",
    "                while(lines[i][j]!='%'):\n",
    "                    if(ord(lines[i][j])>=48 and ord(lines[i][j])<=57):\n",
    "                        per *= 10\n",
    "                        per +=(ord(lines[i][j])-48)\n",
    "                    j+=1\n",
    "            j+=1\n",
    "        temp = -1\n",
    "        array = [per]\n",
    "        while(lines[i][j]!=')'):\n",
    "            if(ord(lines[i][j])>=48 and ord(lines[i][j])<=57):\n",
    "                if(temp==-1):\n",
    "                    temp=0\n",
    "                temp*=10\n",
    "                temp+=(ord(lines[i][j])-48)\n",
    "            else:\n",
    "                if(temp!=-1):\n",
    "                    array.append(temp)\n",
    "                    temp=-1\n",
    "            j+=1\n",
    "        array.append(temp)\n",
    "        boxes[filepath].append(array)\n",
    "    #print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function that checks if two bounding boxes are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(dim1,dim2):\n",
    "    flag = False\n",
    "    if(((dim1[1]<=dim2[1] and dim2[1]<=dim1[1]+dim1[3]) or (dim2[1]<=dim1[1] and dim1[1]<=dim2[1]+dim2[3]))\n",
    "       and ((dim1[2]<=dim2[2] and dim2[2]<=dim1[2]+dim1[4]) or (dim2[2]<=dim1[2] and dim1[2]<=dim2[2]+dim2[4]))):\n",
    "        flag = True\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two bounding boxes overlap, then priority is given to the bounding box with higher detection accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in boxes.keys():\n",
    "    dimensions = boxes[file]\n",
    "    i = 0\n",
    "    reject = []\n",
    "    while(i<len(dimensions)):\n",
    "        j = i+1\n",
    "        while(j<len(dimensions)):\n",
    "            if(overlap(dimensions[i],dimensions[j])):\n",
    "                if(dimensions[i][0]<dimensions[j][0]):\n",
    "                    reject.append(i)\n",
    "                else:\n",
    "                    reject.append(j)\n",
    "            j+=1\n",
    "        i+=1\n",
    "    i = 0\n",
    "    reject = list(dict.fromkeys(reject))\n",
    "    reject.sort(reverse = True)\n",
    "    while(i<len(reject)):\n",
    "        dimensions.pop(reject[i])\n",
    "    boxes[file] = dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will use the dictionary boxes and will splice the image. Then a new image is created that has only the number plate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "for file in boxes.keys():\n",
    "    i = 0\n",
    "    while(i<len(boxes[file])):\n",
    "        x1 = boxes[file][i][1]\n",
    "        x2 = boxes[file][i][1]+boxes[file][i][3]\n",
    "        y1 = boxes[file][i][2]\n",
    "        y2 = boxes[file][i][2]+boxes[file][i][4]\n",
    "        im = cv2.imread(file)\n",
    "        plate = im[y1:y2,x1:x2]\n",
    "        array = file.split('/')\n",
    "        image_name = array[-1:]\n",
    "        image_name = image_name[0]\n",
    "        image_name = image_name[:-4]\n",
    "        image_name+=\"_\"\n",
    "        image_name+=str(i)\n",
    "        image_name+=\".jpg\"\n",
    "        cv2.imwrite(\"exp_recognition/predict_images/\"+image_name,plate)\n",
    "        i+=1\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster-Rcnn-Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting the characters of the number plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL  \n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy72mWwAWKMK"
   },
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v7m_NY_aWKMK"
   },
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bm0_uNRnWKMN"
   },
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VyPz_t8WWKMQ"
   },
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "# MODEL_NAME = 'inference_graph/frozen_inference_graph.pb'\n",
    "# MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "# DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = 'faster_rcnn_inception_v2/inference_graph/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = 'faster_rcnn_inception_v2/labelmap.pbtxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBcB9QHLWKMU"
   },
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KezjCRVvWKMV"
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hDbpHkiWWKMX"
   },
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFsoUHvbWKMZ"
   },
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aSlYc3JkWKMa"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jG-zn5ykWKMd"
   },
   "outputs": [],
   "source": [
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = 'exp_recognition/predict_images'\n",
    "# TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
    "par = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
    "TEST_IMAGE_PATHS = []\n",
    "i = 0\n",
    "for file in par:\n",
    "    if(file[-3:] == 'jpg'):\n",
    "        TEST_IMAGE_PATHS.append(os.path.join(PATH_TO_TEST_IMAGES_DIR,'{}'.format(file)))\n",
    "        i += 1\n",
    "#     if(i == 10): break\n",
    "#     img = cv2.imread(DATA_PATH+\"/train/Parasitized/{}\".format(file))\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (360, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "92BHxzcNWKMf"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster_rcnn_inception model is used on the new images that were generated from the previous model.The results are stored in the directory fasterrcnn_predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3a5wMHN8WKMh"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  image = image.resize((image.size[0]*3, image.size[1]*3), Image.ANTIALIAS)\n",
    "#   print(image.size)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=1)\n",
    "#   plt.figure(figsize=IMAGE_SIZE)\n",
    "#   cv2.imshow(\"image\",image_np)\n",
    "#   cv2.namedWindow('image',WINDOW_NORMAL)\n",
    "#   cv2.resizeWindow('image', 600,600)\n",
    "#   cv2.waitKey(0)\n",
    "  image_name = image_path.split('/')\n",
    "  image_name = image_name[len(image_name)-1]\n",
    "  image_name = image_name.split('.')\n",
    "  image_name = image_name[0]\n",
    "  im = Image.fromarray(image_np)\n",
    "  im.save(\"fasterrcnn_predictions/\"+image_name+\".png\")\n",
    "  #print(output_dict['detection_classes'])\n",
    "  i += 1\n",
    "  #print(i)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LQSEnEsPWKMj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
